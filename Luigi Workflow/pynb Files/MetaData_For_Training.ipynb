{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#from xgboost.sklearn import XGBClassifier\n",
    "import pickle\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def metadata_model():\n",
    "    \n",
    "    print('Inside the function')\n",
    "    ### reading all the 7 csv files\n",
    "    promoted_content = pd.read_csv('promoted_content.csv')\n",
    "    document_category = pd.read_csv('documents_categories.csv')\n",
    "    document_topic = pd.read_csv('documents_topics.csv')\n",
    "    document_entity = pd.read_csv('documents_entities.csv')\n",
    "    events_data = pd.read_csv('events.csv')\n",
    "    clicks_train  = pd.read_csv('clicks_train.csv')\n",
    "    clicks_test = pd.read_csv('clicks_test.csv')\n",
    "    \n",
    "    ### Split the DataFrame into smaller chunks\n",
    "    size_of_chunk = 15000000\n",
    "    Df_List = list()\n",
    "    numOfChunks = len(clicks_train) // size_of_chunk + 1\n",
    "    for i in range(numOfChunks):\n",
    "        Df_List.append(clicks_train[i*size_of_chunk:(i+1)*size_of_chunk])\n",
    "        \n",
    "        \n",
    "    split_train_frame = Df_List\n",
    "    ## reading the first chunk\n",
    "    chunk_frame = split_train_frame[0]\n",
    "    \n",
    "    ## merge chunk frame with events data.\n",
    "    merged_df_with_Event = pd.DataFrame(events_data)\n",
    "    merged_df_with_Event.drop(['uuid','timestamp','document_id'],axis=1,inplace=True)\n",
    "    merged_df_with_Event['Country']=merged_df_with_Event['geo_location'].str[0:2]\n",
    "    merged_df_with_Event.drop(['geo_location'],axis=1,inplace=True)\n",
    "    merged_df_with_Events = pd.merge(chunk_frame,merged_df_with_Event,how='left',on=(['display_id']))\n",
    "    merged_df_with_Events['Country'] = pd.Categorical.from_array(merged_df_with_Events.Country).labels\n",
    "    merged_df_with_Events = merged_df_with_Events.query(\"Country == Country\")\n",
    "    merged_df_with_Events['platform'] = pd.Categorical.from_array(merged_df_with_Events.platform).labels\n",
    "    merged_df_with_Events['platform'] = pd.to_numeric(merged_df_with_Events['platform'], errors='ignore')\n",
    "    \n",
    "    #merged_df_with_Events.to_csv('merged_df_with_Events.csv',index=None)\n",
    "    \n",
    "    merged_df = pd.merge(merged_df_with_Events,promoted_content,how='left',on='ad_id')\n",
    "    \n",
    "    ## merge the above data with document_category data\n",
    "    #merged_df_with_CL_Cat = merge_data_with_document_category(document_category,merged_df)\n",
    "    \n",
    "    average_category_confidenceLevel = pd.DataFrame(document_category.groupby(['document_id'])['confidence_level'].mean()).reset_index()\n",
    "    merged_df_with_CL_Cat = pd.merge(merged_df,average_category_confidenceLevel,how='left',on='document_id')\n",
    "    merged_df_with_CL_Cat = merged_df_with_CL_Cat[np.isfinite(merged_df_with_CL_Cat['confidence_level'])]\n",
    "    merged_df_with_CL_Cat['confidence_level'] = merged_df_with_CL_Cat['confidence_level'].map('{:,.3f}'.format)\n",
    "    merged_df_with_CL_Cat[['confidence_level']] = merged_df_with_CL_Cat[['confidence_level']].astype(float)\n",
    "    gc.collect()\n",
    "    ## grouping confidence level by document_id\n",
    "    average_topic_confidenceLevel = pd.DataFrame(document_topic.groupby(['document_id'])['confidence_level'].mean()).reset_index()\n",
    "    merged_df_with_CL_Top = pd.merge(merged_df_with_CL_Cat,average_topic_confidenceLevel,how='left',on='document_id')\n",
    "    merged_df_with_CL_Top = merged_df_with_CL_Top[np.isfinite(merged_df_with_CL_Top['confidence_level_y'])]\n",
    "    merged_df_with_CL_Top['confidence_level_y'] = merged_df_with_CL_Top['confidence_level_y'].map('{:,.3f}'.format)\n",
    "    merged_df_with_CL_Top[['confidence_level_y']] = merged_df_with_CL_Top[['confidence_level_y']].astype(float)\n",
    "\n",
    "    \n",
    "    ## merge the above data with document_topic\n",
    "    #merged_df_with_CL_Top = merge_data_with_document_topics(document_topic,merged_df_with_CL_Cat)\n",
    "    average_entity_confidenceLevel = pd.DataFrame(document_entity.groupby(['document_id'])['confidence_level'].mean()).reset_index()\n",
    "    merged_df_with_CL_Ent = pd.merge(merged_df_with_CL_Top,average_entity_confidenceLevel,how='left',on='document_id')\n",
    "    merged_df_with_CL_Ent.to_csv(\"merged_df_with_CL_Ent.csv\")\n",
    "    merged_df_with_CL_Ent = pd.read_csv(\"merged_df_with_CL_Ent.csv\")\n",
    "    merged_df_with_CL_Ent = merged_df_with_CL_Ent[np.isfinite(merged_df_with_CL_Ent['confidence_level'])]\n",
    "    merged_df_with_CL_Ent['confidence_level'] = merged_df_with_CL_Ent['confidence_level'].map('{:,.3f}'.format)\n",
    "    merged_df_with_CL_Ent[['confidence_level']] = merged_df_with_CL_Ent[['confidence_level']].astype(float)\n",
    "    merged_df_with_CL_Ent[['platform']] = merged_df_with_CL_Ent[['platform']].astype(int)\n",
    "    merged_df_with_CL_Ent.drop(['Unnamed: 0'],axis=1,inplace=True)\n",
    "    merged_df_with_CL_Ent.to_csv(\"Document_Metadata.csv\", index=False)\n",
    "    print(\"creating the model\")\n",
    "\n",
    "    ## Creating the trained model\n",
    "    X = [x for x in merged_df_with_CL_Ent.columns if x not in ['display_id','clicked']]\n",
    "    Y = ['clicked']\n",
    "    #model = XGBClassifier(max_depth=3,n_estimators=400, learning_rate=0.02)\n",
    "    model = RandomForestClassifier(random_state=1, n_estimators=3, min_samples_split=4, min_samples_leaf=2, n_jobs=3)\n",
    "    train_model = model.fit(merged_df_with_CL_Ent[X],merged_df_with_CL_Ent[Y])\n",
    "\n",
    "    ##creating test data..\n",
    "    size_of_chunk_test = 1000\n",
    "    Df_List_test = list()\n",
    "    numOfChunks_test = len(clicks_test) // size_of_chunk_test + 1\n",
    "    for i in range(numOfChunks_test):\n",
    "        Df_List_test.append(clicks_test[i*size_of_chunk_test:(i+1)*size_of_chunk_test])\n",
    "    \n",
    "    split_test_frame = Df_List_test #splitDataFrameIntoSmaller(clicks_test)\n",
    "    test = split_test_frame[0]\n",
    "    \n",
    "    print(\"Almost done\")\n",
    "    merged_df_with_Event['Country'] = pd.Categorical.from_array(merged_df_with_Event.Country).labels\n",
    "    merged_df_with_Event = merged_df_with_Event.query(\"Country == Country\")\n",
    "    merged_df_with_Event['platform'] = pd.Categorical.from_array(merged_df_with_Event.platform).labels\n",
    "    merged_df_with_Event['platform'] = pd.to_numeric(merged_df_with_Event['platform'], errors='ignore')\n",
    "    \n",
    "    chunk_test = pd.merge(test,merged_df_with_Event,how='left',on='display_id')\n",
    "    print(chunk_test.head(1))\n",
    "    chunk_test_promo = pd.merge(chunk_test,promoted_content,how='left',on='ad_id')\n",
    "    \n",
    "    chunk_test_with_CL_Cat = pd.merge(chunk_test_promo,average_category_confidenceLevel,how='left',on='document_id')\n",
    "    chunk_test_with_CL_Cat = chunk_test_with_CL_Cat.fillna(0.0)\n",
    "    \n",
    "    chunk_test_with_CL_Top = pd.merge(chunk_test_with_CL_Cat,average_topic_confidenceLevel,how='left',on='document_id')\n",
    "    chunk_test_with_CL_Top=chunk_test_with_CL_Top.fillna(0.0)\n",
    "    \n",
    "    chunk_test_with_CL_Ent = pd.merge(chunk_test_with_CL_Top,average_entity_confidenceLevel,how='left',on='document_id')\n",
    "    chunk_test_with_CL_Ent=chunk_test_with_CL_Ent.fillna(0.0)\n",
    "    \n",
    "    chunk_test_with_CL_Ent.to_csv('TestData.csv',index=None)\n",
    "    print(\"Saving Test Data Completed!\")\n",
    "    gc.collect()\n",
    "    ## Save the model..\n",
    "    \n",
    "    save_model = pickle.dumps(train_model)\n",
    "    load_model = pickle.loads(save_model)\n",
    "    joblib.dump(train_model, 'xgboost.pkl')\n",
    "    \n",
    "    trained_model = joblib.load('xgboost.pkl')\n",
    "    \n",
    "    split_test_frame = Df_List_test #splitDataFrameIntoSmaller(clicks_test)\n",
    "    test = split_test_frame[0]\n",
    "    predY=[]\n",
    "    \n",
    "    X_test = [x for x in chunk_test_with_CL_Ent.columns if x not in ['display_id','clicked']]\n",
    "    chunk_pred=list(trained_model.predict_proba(chunk_test_with_CL_Ent[X_test]).astype(float)[:,1])\n",
    "    predY += chunk_pred\n",
    "    results=pd.concat((clicks_test,pd.DataFrame(predY)) ,axis=1,ignore_index=True)\n",
    "    #Combine the predicted values with the ids\n",
    "    \n",
    "    results.columns = ['display_id','ad_id','clicked']#Rename the columns\n",
    "    \n",
    "    results.to_csv(\"xgb_results.csv\", index=False)\n",
    "    results_1 = pd.read_csv('xgb_results.csv')\n",
    "    results_1 = results_1.sort_values(by=['display_id','clicked'], ascending=[True, False])\n",
    "    final_file = results_1.groupby('display_id').ad_id.apply(lambda x: \" \".join(map(str,x))).reset_index()\n",
    "    final_file.to_csv(\"XGB_File_400trees_TrainDatafinal.csv\", index=False)\n",
    "    \n",
    "    ##Saving\n",
    "    save_model = pickle.dumps(train_model)\n",
    "    load_model = pickle.loads(save_model)\n",
    "    joblib.dump(train_model, 'xgboost.pkl')\n",
    "    \n",
    "    trained_model = joblib.load('xgboost.pkl')\n",
    "    \n",
    "    \n",
    "    print(\"Completed..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
